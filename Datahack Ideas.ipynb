{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YINS Datahack 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "The following challenges are available to choose from:\n",
    "* 1) Police Misconduct Data (Yale Policy Lab): http://datahack.yale.edu/files/yale-policy-lab.pdf\n",
    "    * Every day a civilian is shot by a police ocer\n",
    "in the United States. While rare as\n",
    "compared to other types of violence, the use of force by the police is of potentially greater\n",
    "importance: Force, utilized improperly or routinely, can erode trust between citizens and\n",
    "their government, prompt disengagement with the law, and shake the very foundation of\n",
    "our democracy.\n",
    "Communities of ocers\n",
    "in Chicago\n",
    "Despite its importance, and the recent attention to police\n",
    "misconduct as a pressing social problem, there is still\n",
    "much we do not know about the phenomenon - including\n",
    "the rates of police misconduct, its distribution, and its\n",
    "root causes. In large part, this is due to a lack of largescale\n",
    "scientific inquiry, which itself results from a lack\n",
    "of data; neither police departments nor the government\n",
    "are compelled to compile and analyze incidents of police\n",
    "(mis)behavior in any systematic fashion.\n",
    "The recent series of high-profile shootings of unarmed\n",
    "citizens, however, has drawn the nation’s gaze to this\n",
    "issue once again, and citizens, policy makers, and policing\n",
    "experts are mobilizing to uncover what data do exist. One such repository comes from\n",
    "Chicago, IL, and contains records of allegations of police misconduct and investigations\n",
    "of police shootings dating back to 1971. The goal of this DataHack is to leverage this\n",
    "database to answer the White House’s call to develop smarter, more data-driven methods\n",
    "to understand and improve policing in the United States, and to reduce the use of force by\n",
    "ocers.\n",
    "Participants will have access to a database - compiled by The Policy Lab and The\n",
    "Justice Collaboratory at Yale Law School - containing more than 150,000 records of alleged\n",
    "police misconduct (built on reports made by both citizens and other police ocers),\n",
    "as well\n",
    "as related datasets - including departmental and (anonymized) personnel records, maps of\n",
    "the city, and details as to its physical and political infrastructure. Hackers will be charged\n",
    "with uncovering actionable insights into the correlates – and possible drivers – of police\n",
    "Date: February, 2017.\n",
    "1\n",
    "2 THE POLICY LAB & THE JUSTICE COLLABORATORY\n",
    "misconduct, and will be evaluated based on the novelty, robustness, and scalability of their\n",
    "finished product, as well as the degree to which it highlights new conceptual threads as the\n",
    "foundation for future research.\n",
    "At A Glance: Possible Guiding Questions\n",
    "Improving our Understanding of Police Misconduct\n",
    "• Is police misconduct a group or networked phenomenon? If so, what do police\n",
    "misconduct networks look like? To what degree do police misconduct networks\n",
    "look and act like social and behavioral networks? For example, do people who\n",
    "work and train together, engage in misconduct together?\n",
    "• What are the behavioral and network-related correlates of police misconduct?\n",
    "Are there particular characteristics by which we can di↵erentiate ocers\n",
    "who\n",
    "do and do not display more serious forms of misconduct?\n",
    "Improving our Prediction of Police Misconduct\n",
    "• How accurately can we predict future aggression by police ocers?\n",
    "Can we\n",
    "achieve top-shelf accuracy and eciency\n",
    "in our predictions, but do so using\n",
    "models that can later be interrogated, so as to help policy makers and researchers\n",
    "better understand the underlying behavioral mechanisms at work\n",
    "(i.e. using something other than machine learning)?\n",
    "• How can we make all of these insights most useful to police departments, in\n",
    "real time? Can we develop visualization tools to aid police departments and\n",
    "citizens in understanding how misconduct concentrates within social networks?\n",
    "Improving our Study of Police Misconduct\n",
    "• How can we achieve the objectives listed above, but avoid over-fitting? These\n",
    "data come from one city (Chicago), within which some patterns may be idiosyncratic.\n",
    "Can we build models that are likely to scale to other cities?\n",
    "• How can we estimate the scale and contents of missing data? These allegations\n",
    "do not represent every incident of police misconduct – rather, they reflect only\n",
    "those that were ocially\n",
    "reported - a time and energy intensive process, which\n",
    "also requires that citizens trust the police in the first place.\n",
    "    \n",
    "    \n",
    "* 2) Leveraging Social Networks in Driving Corporate Performance (McChrystal Group): http://datahack.yale.edu/files/mcchrystal-group.pdf\n",
    "    * The corporate world is awash with buzz words—organizations live in fear of black swan events, often desperate to stay ahead of disruptors. No executive wants to be featured in the Kodak- or Blockbusterstyle case studies covered by the core curriculum of nearly every MBA. Wall Street analysts, private equity investors, and companies themselves are devoting unprecedented resources to dissecting the numbers behind what makes for a successful organization. But one area that remains under-studied is what creates organizational success in the first place: the people and how those people really interact. As more and more companies move beyond simple line and block hierarchical reporting into crossfunctional, layered matrices, executives are looking to leverage their companies’ internal networks to drive organizational performance. Though organizational network analysis has established itself as a major field in academia, it is still burgeoning in the world of corporate organizational design. Analysts studying networks in the corporate world face unique challenges. Corporate management often lacks the understanding of how to best utilize network analysis findings, in worst-case scenarios using them to make performance management decisions rather than insightful re-alignments of their organization’s networking and communications. As such, the data that analysts obtain from surveys are often anonymous to protect employee identity, making the linking of a respondent to his/her exact networks difficult. Additionally, given a world in which we are asked to rate our experience every time we take a flight or order a pizza, analysts must cope with increasing survey fatigue, and hence, less complete datasets. Facing all of these challenges, we are asking hackers to **leverage several organizational performance and network analysis datasets collected from a variety of industries in order to better understand the nature of social network influence in the corporate world**. More specifically, looking across datasets from companies diverse in size, location, revenue, and industry, **what kind of impact can influencers have on both their peers as well as the company’s overall success**? What **actionable insights can we derive from these datasets in order to improve a company’s performance**? We ask Hackers to offer their **perceptions of the generalizability/scalability of their findings**. **Do they believe that their insights and selected implementations are unique to the datasets they are working with**, or **do they believe these trends could be consistent for an entire corporation, across industry, etc.**? \n",
    "    \n",
    "    \n",
    "* 3) Identifying Groups of Traders that Manipulate the Financial Markets (Goldman Sachs Group): http://datahack.yale.edu/files/goldman-sachs.pdf\n",
    "    * It is a common practice for the Compliance Departments within big financial organizations to **set the appropriate controls and procedures to ensure that employees comply** with applicable rules and regulations. For example, \"Spoofing\" is a practice in which traders attempt to give an artificial impression of market conditions by entering and quickly canceling large buy or sell orders onto an exchange, in an attempt to manipulate prices. The 2010 Dodd-Frank Act specifically forbids spoofing. Monitoring and identifying spoofing at an individual trader level is straightforward; however, finding a group of traders that manipulate the market is more involved. One of the mandates of a contemporary compliance officer is to **identify potential group spoofing activities**, which we define as follows: Definition: [Potential Group Spoofing Activity (PGSA)]: We say that **there is a Potential Group Spoofing Activity when two traders trade the same financial instrument (e.g., a stock) at some timestamp t and they communicate (for example, via email or phone) at the same timestamp t**. In this datahack, we are going to explore methodologies that identify Potential Group Spoofing Activities within big financial organizations. Dataset: We are given **trading data and communication data - all corresponding to a single day**. 1. Trading data: **500 traders trade (buy/sell positions) 1,000 stocks in 100 different timestamps**. 2. Communication data: **500 traders communicate with each other in the same 100 timestamps**. 1. **Determine if there is at least one PGSA at each timestamp**. 2. Find the **timestamp where the fewest PGSAs occur**. 3. **Find all the PGSAs** in this dataset. 4. Find the **“riskiest” PGSA** and explain your reasoning in detail. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "police: complaint data, and co-complaints. links are co-complains over a series of time. timeseries data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police Misconduct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.sparse import linalg, csr_matrix\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def read_network(fn, undirected = True):\n",
    "    edges = list()\n",
    "    nodes_id = dict()\n",
    "    node_counter = -1\n",
    "    with open(fn, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            nodei, nodej = line.rstrip('\\n').split()\n",
    "            if nodei not in nodes_id:\n",
    "                node_counter += 1\n",
    "                nodes_id[nodei] = node_counter\n",
    "            if nodej not in nodes_id:\n",
    "                node_counter += 1\n",
    "                nodes_id[nodej] = node_counter\n",
    "            \n",
    "            edges.append((nodei, nodej))\n",
    "            \n",
    "    A = np.zeros((node_counter + 1, node_counter + 1), dtype = float)\n",
    "    for e in edges:\n",
    "        i = nodes_id[e[0]]\n",
    "        j = nodes_id[e[1]]\n",
    "        A[i, j] = 1.0\n",
    "        if undirected:\n",
    "            A[j, i] = 1.0\n",
    "     \n",
    "    return A, node_counter + 1\n",
    "# dA, dN = read_network('data/smallnet2.txt', undirected = False) \n",
    "def get_centrality(G, type_centrality):\n",
    "    \n",
    "    if type_centrality == \"degree\":\n",
    "        return nx.degree_centrality(G)\n",
    "        \n",
    "    elif type_centrality == \"closeness\":\n",
    "        return nx.closeness_centrality(G)\n",
    "    \n",
    "    elif type_centrality == \"betweenness\":\n",
    "        return nx.betweenness_centrality(G)\n",
    "    \n",
    "    elif type_centrality == \"eigenvector\":\n",
    "        return nx.eigenvector_centrality(G, max_iter = 1000, tol = 1e-06)\n",
    "    \n",
    "    elif type_centrality == \"katz\":\n",
    "        return nx.katz_centrality(G, alpha = 0.01, beta = 0.01, max_iter = 1000, tol = 1e-06)\n",
    "    \n",
    "    elif type_centrality == \"pagerank\":\n",
    "        return nx.pagerank(G, 0.85)\n",
    "    else:\n",
    "        print \"wrong type of centrality\"\n",
    "        return None\n",
    "    \n",
    "def get_centrality_distribution(centrality, number_of_bins = 50, log_binning = False, base = 10):\n",
    "    if type(centrality) == dict:\n",
    "        centrality = centrality.values()\n",
    "    elif type(centrality) == list:\n",
    "        centrality = centrality\n",
    "    else:\n",
    "        print \"wrong type of centrality. must be either dictionary or list\"\n",
    "        return None, None\n",
    "    \n",
    "    # We need to define the support of our distribution\n",
    "    lower_bound = min(centrality)\n",
    "    upper_bound = max(centrality)\n",
    "    \n",
    "    # And the bins\n",
    "    if log_binning:\n",
    "        log = np.log2 if base == 2 else np.log10\n",
    "        lower_bound = log(lower_bound) if lower_bound > 0 else -1\n",
    "        upper_bound = log(upper_bound)\n",
    "        bins = np.logspace(lower_bound, upper_bound, number_of_bins, base = base)\n",
    "    else:\n",
    "        bins = np.linspace(lower_bound,upper_bound,number_of_bins)\n",
    "        \n",
    "    # Then we can compute the histogram using numpy\n",
    "    y, __ = np.histogram(centrality, \n",
    "                           bins = bins,\n",
    "                           density = True)\n",
    "    #print centrality\n",
    "    # Now, we need to compute for each y the value of x\n",
    "    x = bins[1:] - np.diff(bins) / 2.0\n",
    "        \n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import scipy.stats as stats\n",
    "#stats.kendalltau is for two arrays. We have centralities as dictionary\n",
    "\n",
    "def k_tau(dict1, dict2):\n",
    "    list_1 = sorted(dict1.items(), key = itemgetter(1), reverse = True)\n",
    "    list_2 = sorted(dict2.items(), key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    id1 = map(itemgetter(0), list_1)\n",
    "    id2 = map(itemgetter(0), list_2)\n",
    "    \n",
    "    kendall_tau, p_value = stats.kendalltau(id1, id2)\n",
    "    \n",
    "    return kendall_tau, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "color_pool = ['r', 'g']\n",
    "for idx, method in enumerate(['pagerank', 'katz']):\n",
    "    centralities = get_centrality(net, method)\n",
    "    x, y = get_centrality_distribution(centralities, number_of_bins = 50, log_binning = True, base = 10)\n",
    "    plt.loglog(x, y, 'o', color = color_pool[idx], label = method)\n",
    "\n",
    "plt.xlabel(\"Centrality\")\n",
    "plt.ylabel(\"P\")\n",
    "plt.legend(numpoints = 1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "full_misconduct_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
